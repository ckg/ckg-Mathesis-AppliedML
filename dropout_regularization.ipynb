{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Γκολφ, Θερισμός, Ομαλοποίηση\n",
    "\n",
    "Προσαρμοσμένο από [την αντίστοιχη τεκμηρίωση του TensorFlow](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit).\n",
    "\n",
    "---\n",
    "\n",
    "> Πάνος Λουρίδας, Αναπληρωτής Καθηγητής <br />\n",
    "> Τμήμα Διοικητικής Επιστήμης και Τεχνολογίας <br />\n",
    "> Οικονομικό Πανεπιστήμιο Αθηνών <br />\n",
    "> louridas@aueb.gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(0) # for replicability purposes, not for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Τα δεδομένα που θα χρησιμοποιήσουμε προέρχονται από μία έρευνα όπου εξετάστηκαν οι δυνατότητες νευρωνικών δικτύων στην αναζήτηση νέων σωματιδίων στη φυσική: Baldi, P., Sadowski, P. & Whiteson, D. Searching for exotic particles in high-energy physics with deep learning. Nat Commun 5, 4308 (2014). https://doi.org/10.1038/ncomms5308.\n",
    "\n",
    "* Το σύνολο των δεδομένων είναι διαθέσιμο από το <https://archive.ics.uci.edu/ml/datasets/HIGGS>. Εμείς για πρακτικούς λόγους θα χρησιμοποιήσουμε ένα υποσύνολο με 13.000 δείγματα.\n",
    "\n",
    "* Ο σκοπός είναι να κάνουμε ταξινόμηση μεταξύ δύο κλάσεων (στήλη `label`, αν υπάρχει ένδειξη ή όχι) με βάση 28 χαρακτηριστικά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "higgs_data = pd.read_csv(\"data/higgs.csv\", dtype=float)\n",
    "higgs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα πάρουμε 11.000 δείγματα για εκπαίδευση και επικύρωση και θα χρησιμοποιήσουμε 2.000 δείγματα για έλεγχο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(higgs_data.iloc[:, 1:], \n",
    "                                                    higgs_data.iloc[:, 0], \n",
    "                                                    train_size=11000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα εξασφαλίσουμε ότι οι τιμές των χαρακτηριστικών έχουν μέση τιμή μηδέν και διακύμανση ένα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "normalizer = layers.Normalization()\n",
    "normalizer.adapt(np.array(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Στο μοντέλο μας θα χρησιμοποιήσουμε μία συνάρτηση ενεργοποίησης που δεν έχουμε ξαναδεί, την Εκθετική Γραμμική Μονάδα, ELU (Exponential Linear Unit):\n",
    "\n",
    "$$\n",
    "  f(x) = \n",
    "  \\begin{cases} \n",
    "   x &  x > 0 \\\\\n",
    "   \\alpha (e^{x} - 1) &  x \\le 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "* Η ELU διαφέρει από τη ReLU στο ότι είναι πιο ομαλή κοντά στο σημείο μηδέν και επιπλέον μπορεί να δώσει αρνητική έξοδο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"text.usetex\"] = True\n",
    "\n",
    "elu_label = (\n",
    "    r\"\\begin{eqnarray*}\"\n",
    "    r\"f(x) &=& x \\quad x > 0 \\\\\"\n",
    "    r\"f(x) &=& \\alpha (e^{x} - 1)\\quad  x \\le 0\"\n",
    "    r\"\\end{eqnarray*}\"\n",
    ")\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xlim((-5, 5))\n",
    "plt.xticks(np.arange(-5, 6, 1))\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.zeros_like(x)\n",
    "y = x.copy()\n",
    "alpha = 1\n",
    "y[0:50] = alpha * (np.exp(x[0:50]) - 1)\n",
    "ax.text(-4.5, 5, elu_label, color=\"k\", fontsize=16)\n",
    "_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα ορίσουμε τον αριθμό των εποχών, το μέγεθος κάθε φουρνιάς, και από εκεί τον αριθμό των βημάτων εκπαίδευσης σε κάθε εποχή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10000\n",
    "BATCH_SIZE = 100\n",
    "STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Μία σημαντική υπερπαράμετρος στα νευρωνικά δίκτυα είναι ο *ρυθμός εκμάθησης* (learning rate).\n",
    "\n",
    "* Αυτός ορίζει πόσο μεγάλες θα είναι οι διορθώσεις που γίνονται στα βάρη και στις πολώσεις σε κάθε βήμα της εκμάθησης.\n",
    "\n",
    "* Αν ο ρυθμός εκμάθησης είναι μεγάλος, τότε οι διορθώσεις είναι μεγάλες, και το δίκτυο μαθαίνει γρήγορα.\n",
    "\n",
    "* Από την άλλη, μπορεί να είναι «απρόσεχτο»: οι διορθώσεις να είναι τόσο μεγάλες, ώστε να ξεφύγει από τις βέλτιστες τιμές των βαρών και των πολώσεων.\n",
    "\n",
    "* Αν ο ρυθμός εκμάθησης είναι μικρός, τότε οι διορθώσεις είναι μικρότερες, το δίκτυο μαθαίνει πιο συντηρητικά, και χρειάζεται περισσότερος χρόνος για την εκπαίδευση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ένας τρόπος να το προσεγγίσουμε αυτό είναι να σκεφτούμε τι γίνεται στο γκολφ.\n",
    "\n",
    "* Στην αρχή ο παίκτης είναι μακριά από την τρύπα και τα χτυπήματά του θα είναι δυνατά, ώστε γρήγορα η μπάλα να προσεγγίσει την τρύπα.\n",
    "\n",
    "* Όταν όμως πλησιάσει στην τρύπα, ο παίκτης γίνεται πιο προσεκτικός. Αν συνεχίσει να χτυπάει με τον ίδιο τρόπο, η μπάλα απλώς θα φεύγει από την άλλη μεριά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Αν χρησιμοποιήσουμε τον βελτιστοποιητή Adam (ή και άλλους), μπορούμε να ορίσουμε ένα μεταβλητό ρυθμό εκμάθησης.\n",
    "\n",
    "* Συγκεκριμένα, θα ορίσουμε ένα *πρόγραμμα εκμάθησης* (learning schedule), στο οποίο o ρυθμός εκμάθησης θα μειώνεται βαθμιαία.\n",
    "\n",
    "* Η ιδέα είναι ότι αρχικά θέλουμε το νευρωνικό μας δίκτυο να μαθαίνει γρήγορα, αλλά όσο προχωράει η εκπαίδευση θέλουμε να γίνεται πιο προσεκτικό."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Χρησιμοποιώντας την κλάση `InverseTimeDecay` ορίζουμε ότι θα ξεκινήσουμε με έναν αρχικό ρυθμό εκμάθησης και στη συνέχεια κάθε 100 εποχές αυτός θα μειώνεται ώστε στις 100 εποχές να γίνει 1/2 του αρχικού, στις 200 το 1/3 του αρχικού, κ.ο.κ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH * 100,\n",
    "  decay_rate=1,\n",
    "  staircase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Φτιάχνουμε τώρα το μοντέλο μας, όπου θα χρησιμοποιήσουμε όπως στην αρχική δημοσίευση στρώματα των 300 νευρώνων.\n",
    "\n",
    "* Στο τέλος θα έχουμε ένα στρώμα με σιγμοειδή συνάρτηση ενεργοποίησης, αφού θέλουμε ταξινόμηση μεταξύ δύο κλάσεων.\n",
    "\n",
    "* Η απώλεια θα υπολογίζεται μέσω της κλάσης `BinaryCrossentropy` και ως μετρικές θα έχουμε την`BinaryAccuracy` και την `BinaryCrossEntropy` (θα επανέλθουμε αργότερα σε αυτό το σημείο)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dense(300, activation='elu'),    \n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "              metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy'),\n",
    "                  tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Να το δούμε συνοπτικά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Θα εκπαιδεύσουμε το μοντέλο μας μέχρι 10000 εποχές.\n",
    "\n",
    "* Αντί να εμφανίζουμε την πρόοδο ανά μία εποχή (πάρα πολλές), θα εμφανίζουμε την πρόοδο ανά 100 εποχές, ενώ κάθε μία εποχή που ολοκληρώνεται θα τυπώνουμε απλώς μια τελεία.  \n",
    "\n",
    "* Για να το κάνουμε αυτό θα ορίσουμε μία δική μας κλάση που θα δώσουμε στο TensorFlow ως callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# From https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/modeling/__init__.py\n",
    "class EpochDots(tf.keras.callbacks.Callback):\n",
    "    \"\"\"A simple callback that prints a \".\" every epoch, with occasional reports.\n",
    "    \n",
    "    Args:\n",
    "        report_every: How many epochs between full reports\n",
    "        dot_every: How many epochs between dots.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, report_every=100, dot_every=1):\n",
    "        self.report_every = report_every\n",
    "        self.dot_every = dot_every\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % self.report_every == 0:\n",
    "            print()\n",
    "            print('Epoch: {:d}, '.format(epoch), end='')\n",
    "            for name, value in sorted(logs.items()):\n",
    "                print('{}:{:0.4f}'.format(name, value), end=',  ')\n",
    "            print()\n",
    "\n",
    "        if epoch % self.dot_every == 0:\n",
    "            print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Και τώρα προχωράμε στην εκπαίδευση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = [\n",
    "    EpochDots(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Μπορούμε να δούμε την εξέλιξη αν φτιάξουμε μια βοηθητική συνάρτηση για την απεικόνιση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['binary_crossentropy'], label='training')\n",
    "    plt.plot(history.history['val_binary_crossentropy'], label='validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary Crossentropy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Τα αποτελέσματα είναι τραγωδία.\n",
    "\n",
    "* Πολύ γρήγορα η απώλεια στην επικύρωση εκτοξεύεται.\n",
    "\n",
    "* Αυτό είναι σαφής ένδειξη υπερπροσαρμογής."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Ένας τρόπος να καταπολεμήσουμε την υπεπροσαρμογή στα νευρωνικά δίκτυα είναι μέσω του *dropout*, το οποίο ίσως στα ελληνικά θα μπορούσαμε να αποδώσουμε ως «θερισμός».\n",
    "\n",
    "* Η [ιδέα πίσω από το dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) είναι ότι σε κάθε βήμα της εκπαίδευσης, οι κόμβοι «απορρίπτονται» με πιθανότητα $p$ ή διατηρούνται με πιθανότητα $1 - p$.\n",
    "\n",
    "* Στην πράξη δηλαδή, θερίζουμε από το νευρωνικό δίκτυο τους κόμβους με πιθανότητα $p$.\n",
    "\n",
    "* Στο επόμενο βήμα της εκπαίδευσης, οι κόμβοι που θα απορριφθούν πάλι επιλέγονται τυχαία, άρα κάποιοι θα ξαναεισαχθούν και κάποιοι άλλοι θα βγουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(300, input_shape=(X_train.shape[1],), activation='elu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(300, activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "              metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy'),\n",
    "                  tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Βλέπουμε ότι η κατάσταση έχει κάπως βελτιωθεί."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Μια άλλη ιδέα για να αποφύγουμε την υπερπροσαρμογή είναι η *ομαλοποίηση* (regularization).\n",
    "\n",
    "* Στην προσέγγιση αυτή, ελαττώνουμε τα βάρη προς το μηδέν. \n",
    "\n",
    "* Ο λόγος είναι ότι όσο λιγότερα βάρη στο τελικό μοντέλο, τόσο λιγότερες παραμέτρους έχουμε στο τέλος, άρα τόσο πιο λιτό είναι το τελικό μοντέλο."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Στην ομαλοποίηση L1, προσθέτουμε στην απώλεια μια ποσότητα ανάλογη με την απόλυτη τιμή του κάθε βάρους. Το αποτέλεσμα είναι να μηδενίζονται κάποια βάρη.\n",
    "\n",
    "* Στην ομαλοποίηση L2, προσθέτουμε στην απώλεια μια ποσότητα ανάλογη με το τετράγωνο του κάθε βάρους. Το αποτέλεσμα είναι να ελαχιστοποιούνται (αλλά όχι να μηδενίζονται απολύτως) κάποια βάρη."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ειδικότερα, αν χρησιμοποιήσουμε `regularizers.l2(0.001)`, όπως παρακάτω, κάθε βάρος θα προσθέσει $0{,}001 \\times w^2$ στην απώλεια του δικτύου.\n",
    "\n",
    "* Αυτός είναι και ο λόγος που παρακολουθούμε την μετρική `binary_crossentropy` και όχι απλώς την απώλεια, γιατί δεν θέλουμε να λαμβάνουμε υπόψη την ομαλοποίηση.\n",
    "\n",
    "* Τώρα στην έξοδο, τα `binary_crossentropy`, `val_binary_cross_entropy` θα διαφέρουν από τα `loss` και `val_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(300, input_shape=(X_train.shape[1],),\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "              metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy'),\n",
    "                  tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Βλέπουμε ότι και η ομαλοποίηση μπορεί να βοηθήσει."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Οπότε, το επόμενο λογικό βήμα είναι να συνδυάσουμε και τις δύο μεθόδους για την αποφυγή της υπερπροσαρμογής."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(300, input_shape=(X_train.shape[1],),\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(300,\n",
    "                 kernel_regularizer=regularizers.l2(0.001),\n",
    "                 activation='elu'),\n",
    "    layers.Dropout(0.5),    \n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "              metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy'),\n",
    "                  tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Πετύχαμε ακόμα καλύτερη συμπεριφορά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Μπορούμε επιπλέον να αξιολογήσουμε το μοντέλο μας με βάση τα δεδομένα ελέγχου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "for metric_name, metric in zip(model.metrics_names, metrics):\n",
    "    print(metric_name, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Η εφαρμογή σωστής στρατηγικής για την αποφυγή υπερπροσαρμογής είναι βασικό συστατικό επιτυχίας σε ένα μοντέλο νευρωνικού δικτύου.\n",
    "\n",
    "* Τα εργαλεία μας δίνουν τη δυνατότητα να στοιβάζουμε στρώματα επί στρωμάτων.\n",
    "\n",
    "* Αυτό δεν σημαίνει ότι έτσι θα λύσουμε το πρόβλημα.\n",
    "\n",
    "* Πρέπει να προσέχουμε πάντα να χρησιμοποιούμε σωστά τη δύναμη που μας δίνουν τα εργαλεία που έχουμε στα χέρια μας."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
